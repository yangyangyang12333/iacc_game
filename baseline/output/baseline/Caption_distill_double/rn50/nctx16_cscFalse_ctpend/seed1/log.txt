***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/Caption_distill_double/rn50.yaml
dataset_config_file: configs/datasets/pazhou_distill_chatglm.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.Caption.N_CTX', '16', 'TRAINER.Caption.CSC', 'False', 'TRAINER.Caption.CLASS_TOKEN_POSITION', 'end']
output_dir: output/baseline/Caption_distill_double/rn50/nctx16_cscFalse_ctpend/seed1
resume: 
root: ./
seed: 1
source_domains: None
target_domains: None
trainer: Caption_distill_double
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 256
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 512
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: pazhou_distill_chatglm
  NUM_LABELED: -1
  NUM_SHOTS: -1
  ROOT: ./
  SAMPLE: 0
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
  partial_prob: 0.5
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
  TRANSFORMS_TEST: ('resize', 'center_crop', 'normalize')
  cutout_proportion: 0.4
  random_resized_crop_scale: (0.6, 1.0)
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.0002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/baseline/Caption_distill_double/rn50/nctx16_cscFalse_ctpend/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: MLClassification
  EVALUATOR_ACT: default_merge_aux
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SAVE_PREDS: 
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  Caption_num: 0
  IF_LEARN_SCALE: False
  IF_LEARN_spatial_SCALE: False
  IF_ablation: False
  LOSSFUNC: double_ranking
  PRINT_FREQ: 5
  spatial_SCALE_image: 50
  spatial_SCALE_text: 50
TRAINER:
  CG:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  Caption:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    DeFo_fix_kweight: True
    DeFo_init_clsname: True
    DeFo_query_nums: 256
    GL_merge_rate: 0.5
    N_CTX: 16
    PREC: fp32
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEA:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: Caption_distill_double
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.13.1+cu117
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.17

Python version: 3.7.16 (default, Jan 17 2023, 22:20:44)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-76-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: 11.7.99
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 2080 Ti
GPU 1: NVIDIA GeForce RTX 2080 Ti
GPU 2: NVIDIA GeForce RTX 2080 Ti
GPU 3: NVIDIA GeForce RTX 2080 Ti
GPU 4: NVIDIA GeForce RTX 2080 Ti
GPU 5: NVIDIA GeForce RTX 2080 Ti
GPU 6: NVIDIA GeForce RTX 2080 Ti
GPU 7: NVIDIA GeForce RTX 2080 Ti

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.21.5
[pip3] torch==1.13.1
[pip3] torchaudio==0.8.0a0+a751e1d
[pip3] torchvision==0.9.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.1.1              ha002fc5_10    conda-forge
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h402132d_0    conda-forge
[conda] mkl_fft                   1.3.1            py37h3e078e5_1    conda-forge
[conda] mkl_random                1.2.2            py37h219a48f_0    conda-forge
[conda] numpy                     1.21.5           py37h6c91a56_3  
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] torch                     1.13.1                   pypi_0    pypi
[conda] torchaudio                0.8.0                      py37    pytorch
[conda] torchvision               0.9.0                py37_cu111    pytorch
        Pillow (9.5.0)

Loading trainer: Caption_distill_double
Loading dataset: pazhou_distill_chatglm
===== chatglm generate 800 sentences ===== torch.Size([800, 77]) torch.Size([800, 80])
===== Caption Distill Data: 800 nums of word filtered caption  =====
Building transform_train, more augment
+ random resized crop (size=(224, 224), scale=(0.6, 1.0)), chosen with cutout
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
transform_test choices: ('resize', 'center_crop', 'normalize')
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
***** Dataset statistics *****
  Dataset: pazhou_distill_chatglm
  # classes: 80
  # train_x: 800
  # val: 220
  # test: 21,995
==================== Building model in Caption_distill_double ======================
Loading CLIP (backbone: RN50)
Building custom CLIP
Initializing a generic context
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Initial double context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: MLClassification
***** Constructing MLClassification
begin train
No checkpoint found, train from scratch
Initializing summary writer for tensorboard with log_dir=output/baseline/Caption_distill_double/rn50/nctx16_cscFalse_ctpend/seed1/tensorboard
epoch [1/100][1/1]	time 1.740 (1.740)	data 0.671 (0.671)	eta 0:02:52	loss 121.3830 (121.3830)	lr 2.000000e-04
after_epoch
epoch [2/100][1/1]	time 1.162 (1.162)	data 0.513 (0.513)	eta 0:01:53	loss 119.3041 (119.3041)	lr 1.999507e-04
after_epoch
epoch [3/100][1/1]	time 1.169 (1.169)	data 0.527 (0.527)	eta 0:01:53	loss 47.0767 (47.0767)	lr 1.998027e-04
after_epoch
epoch [4/100][1/1]	time 1.160 (1.160)	data 0.508 (0.508)	eta 0:01:51	loss 49.8174 (49.8174)	lr 1.995562e-04
after_epoch
epoch [5/100][1/1]	time 1.319 (1.319)	data 0.662 (0.662)	eta 0:02:05	loss 43.7259 (43.7259)	lr 1.992115e-04
after_epoch
epoch [6/100][1/1]	time 1.164 (1.164)	data 0.512 (0.512)	eta 0:01:49	loss 40.5756 (40.5756)	lr 1.987688e-04
after_epoch
epoch [7/100][1/1]	time 1.350 (1.350)	data 0.703 (0.703)	eta 0:02:05	loss 37.4847 (37.4847)	lr 1.982287e-04
after_epoch
epoch [8/100][1/1]	time 1.186 (1.186)	data 0.534 (0.534)	eta 0:01:49	loss 32.3635 (32.3635)	lr 1.975917e-04
after_epoch
epoch [9/100][1/1]	time 1.170 (1.170)	data 0.513 (0.513)	eta 0:01:46	loss 31.3847 (31.3847)	lr 1.968583e-04
after_epoch
epoch [10/100][1/1]	time 1.188 (1.188)	data 0.544 (0.544)	eta 0:01:46	loss 26.4191 (26.4191)	lr 1.960294e-04
after_epoch
epoch [11/100][1/1]	time 1.165 (1.165)	data 0.521 (0.521)	eta 0:01:43	loss 26.2212 (26.2212)	lr 1.951057e-04
after_epoch
epoch [12/100][1/1]	time 1.249 (1.249)	data 0.595 (0.595)	eta 0:01:49	loss 23.8678 (23.8678)	lr 1.940881e-04
after_epoch
epoch [13/100][1/1]	time 1.179 (1.179)	data 0.529 (0.529)	eta 0:01:42	loss 22.0408 (22.0408)	lr 1.929776e-04
after_epoch
epoch [14/100][1/1]	time 1.170 (1.170)	data 0.525 (0.525)	eta 0:01:40	loss 19.9140 (19.9140)	lr 1.917755e-04
after_epoch
epoch [15/100][1/1]	time 1.130 (1.130)	data 0.486 (0.486)	eta 0:01:36	loss 19.1294 (19.1294)	lr 1.904827e-04
after_epoch
epoch [16/100][1/1]	time 1.162 (1.162)	data 0.509 (0.509)	eta 0:01:37	loss 15.1015 (15.1015)	lr 1.891007e-04
after_epoch
epoch [17/100][1/1]	time 1.206 (1.206)	data 0.560 (0.560)	eta 0:01:40	loss 15.3101 (15.3101)	lr 1.876307e-04
after_epoch
epoch [18/100][1/1]	time 1.169 (1.169)	data 0.510 (0.510)	eta 0:01:35	loss 16.4867 (16.4867)	lr 1.860742e-04
after_epoch
epoch [19/100][1/1]	time 1.206 (1.206)	data 0.553 (0.553)	eta 0:01:37	loss 14.8970 (14.8970)	lr 1.844328e-04
after_epoch
epoch [20/100][1/1]	time 1.169 (1.169)	data 0.509 (0.509)	eta 0:01:33	loss 14.8549 (14.8549)	lr 1.827081e-04
after_epoch
epoch [21/100][1/1]	time 1.180 (1.180)	data 0.523 (0.523)	eta 0:01:33	loss 14.8227 (14.8227)	lr 1.809017e-04
after_epoch
epoch [22/100][1/1]	time 1.185 (1.185)	data 0.539 (0.539)	eta 0:01:32	loss 13.1549 (13.1549)	lr 1.790155e-04
after_epoch
epoch [23/100][1/1]	time 1.165 (1.165)	data 0.512 (0.512)	eta 0:01:29	loss 13.7304 (13.7304)	lr 1.770513e-04
after_epoch
epoch [24/100][1/1]	time 1.171 (1.171)	data 0.522 (0.522)	eta 0:01:29	loss 12.4317 (12.4317)	lr 1.750111e-04
after_epoch
epoch [25/100][1/1]	time 1.175 (1.175)	data 0.519 (0.519)	eta 0:01:28	loss 11.9498 (11.9498)	lr 1.728969e-04
after_epoch
epoch [26/100][1/1]	time 1.213 (1.213)	data 0.560 (0.560)	eta 0:01:29	loss 13.9470 (13.9470)	lr 1.707107e-04
after_epoch
epoch [27/100][1/1]	time 1.181 (1.181)	data 0.525 (0.525)	eta 0:01:26	loss 10.7119 (10.7119)	lr 1.684547e-04
after_epoch
epoch [28/100][1/1]	time 1.156 (1.156)	data 0.496 (0.496)	eta 0:01:23	loss 13.8765 (13.8765)	lr 1.661312e-04
after_epoch
epoch [29/100][1/1]	time 1.164 (1.164)	data 0.505 (0.505)	eta 0:01:22	loss 12.3785 (12.3785)	lr 1.637424e-04
after_epoch
epoch [30/100][1/1]	time 1.202 (1.202)	data 0.559 (0.559)	eta 0:01:24	loss 11.9317 (11.9317)	lr 1.612907e-04
after_epoch
epoch [31/100][1/1]	time 1.192 (1.192)	data 0.535 (0.535)	eta 0:01:22	loss 12.6561 (12.6561)	lr 1.587785e-04
after_epoch
epoch [32/100][1/1]	time 1.190 (1.190)	data 0.541 (0.541)	eta 0:01:20	loss 10.0560 (10.0560)	lr 1.562083e-04
after_epoch
epoch [33/100][1/1]	time 1.291 (1.291)	data 0.644 (0.644)	eta 0:01:26	loss 10.7915 (10.7915)	lr 1.535827e-04
after_epoch
epoch [34/100][1/1]	time 1.302 (1.302)	data 0.639 (0.639)	eta 0:01:25	loss 9.9775 (9.9775)	lr 1.509041e-04
after_epoch
epoch [35/100][1/1]	time 1.354 (1.354)	data 0.696 (0.696)	eta 0:01:28	loss 12.4347 (12.4347)	lr 1.481754e-04
after_epoch
epoch [36/100][1/1]	time 1.334 (1.334)	data 0.686 (0.686)	eta 0:01:25	loss 12.3202 (12.3202)	lr 1.453990e-04
after_epoch
epoch [37/100][1/1]	time 1.231 (1.231)	data 0.578 (0.578)	eta 0:01:17	loss 11.3092 (11.3092)	lr 1.425779e-04
after_epoch
epoch [38/100][1/1]	time 1.195 (1.195)	data 0.545 (0.545)	eta 0:01:14	loss 12.3787 (12.3787)	lr 1.397148e-04
after_epoch
epoch [39/100][1/1]	time 1.273 (1.273)	data 0.628 (0.628)	eta 0:01:17	loss 11.5760 (11.5760)	lr 1.368125e-04
after_epoch
epoch [40/100][1/1]	time 1.182 (1.182)	data 0.518 (0.518)	eta 0:01:10	loss 9.1979 (9.1979)	lr 1.338738e-04
after_epoch
epoch [41/100][1/1]	time 1.243 (1.243)	data 0.582 (0.582)	eta 0:01:13	loss 10.8500 (10.8500)	lr 1.309017e-04
after_epoch
epoch [42/100][1/1]	time 1.166 (1.166)	data 0.508 (0.508)	eta 0:01:07	loss 10.2127 (10.2127)	lr 1.278991e-04
after_epoch
epoch [43/100][1/1]	time 1.187 (1.187)	data 0.538 (0.538)	eta 0:01:07	loss 11.0837 (11.0837)	lr 1.248690e-04
after_epoch
epoch [44/100][1/1]	time 1.218 (1.218)	data 0.560 (0.560)	eta 0:01:08	loss 9.4545 (9.4545)	lr 1.218143e-04
after_epoch
epoch [45/100][1/1]	time 1.188 (1.188)	data 0.525 (0.525)	eta 0:01:05	loss 10.1094 (10.1094)	lr 1.187381e-04
after_epoch
epoch [46/100][1/1]	time 1.212 (1.212)	data 0.554 (0.554)	eta 0:01:05	loss 8.2537 (8.2537)	lr 1.156434e-04
after_epoch
epoch [47/100][1/1]	time 1.210 (1.210)	data 0.547 (0.547)	eta 0:01:04	loss 10.9401 (10.9401)	lr 1.125333e-04
after_epoch
epoch [48/100][1/1]	time 1.218 (1.218)	data 0.571 (0.571)	eta 0:01:03	loss 10.1226 (10.1226)	lr 1.094108e-04
after_epoch
epoch [49/100][1/1]	time 1.214 (1.214)	data 0.568 (0.568)	eta 0:01:01	loss 9.9789 (9.9789)	lr 1.062791e-04
after_epoch
epoch [50/100][1/1]	time 1.224 (1.224)	data 0.564 (0.564)	eta 0:01:01	loss 9.7798 (9.7798)	lr 1.031411e-04
after_epoch
epoch [51/100][1/1]	time 1.178 (1.178)	data 0.518 (0.518)	eta 0:00:57	loss 10.5932 (10.5932)	lr 1.000000e-04
after_epoch
epoch [52/100][1/1]	time 1.189 (1.189)	data 0.534 (0.534)	eta 0:00:57	loss 9.0928 (9.0928)	lr 9.685892e-05
after_epoch
epoch [53/100][1/1]	time 1.195 (1.195)	data 0.537 (0.537)	eta 0:00:56	loss 9.4845 (9.4845)	lr 9.372095e-05
after_epoch
epoch [54/100][1/1]	time 1.276 (1.276)	data 0.621 (0.621)	eta 0:00:58	loss 8.8611 (8.8611)	lr 9.058917e-05
after_epoch
epoch [55/100][1/1]	time 1.171 (1.171)	data 0.519 (0.519)	eta 0:00:52	loss 9.5384 (9.5384)	lr 8.746668e-05
after_epoch
epoch [56/100][1/1]	time 1.161 (1.161)	data 0.505 (0.505)	eta 0:00:51	loss 9.1017 (9.1017)	lr 8.435655e-05
after_epoch
epoch [57/100][1/1]	time 1.184 (1.184)	data 0.519 (0.519)	eta 0:00:50	loss 10.3214 (10.3214)	lr 8.126187e-05
after_epoch
epoch [58/100][1/1]	time 1.159 (1.159)	data 0.508 (0.508)	eta 0:00:48	loss 10.1170 (10.1170)	lr 7.818568e-05
after_epoch
epoch [59/100][1/1]	time 1.199 (1.199)	data 0.543 (0.543)	eta 0:00:49	loss 7.7488 (7.7488)	lr 7.513101e-05
after_epoch
epoch [60/100][1/1]	time 1.178 (1.178)	data 0.517 (0.517)	eta 0:00:47	loss 9.3942 (9.3942)	lr 7.210089e-05
after_epoch
epoch [61/100][1/1]	time 1.190 (1.190)	data 0.542 (0.542)	eta 0:00:46	loss 10.0915 (10.0915)	lr 6.909830e-05
after_epoch
epoch [62/100][1/1]	time 1.210 (1.210)	data 0.568 (0.568)	eta 0:00:45	loss 10.4177 (10.4177)	lr 6.612621e-05
after_epoch
epoch [63/100][1/1]	time 1.485 (1.485)	data 0.829 (0.829)	eta 0:00:54	loss 9.2931 (9.2931)	lr 6.318754e-05
after_epoch
epoch [64/100][1/1]	time 1.172 (1.172)	data 0.525 (0.525)	eta 0:00:42	loss 8.1794 (8.1794)	lr 6.028521e-05
after_epoch
epoch [65/100][1/1]	time 1.184 (1.184)	data 0.528 (0.528)	eta 0:00:41	loss 7.8319 (7.8319)	lr 5.742207e-05
after_epoch
epoch [66/100][1/1]	time 1.189 (1.189)	data 0.532 (0.532)	eta 0:00:40	loss 10.0119 (10.0119)	lr 5.460095e-05
after_epoch
epoch [67/100][1/1]	time 1.159 (1.159)	data 0.514 (0.514)	eta 0:00:38	loss 8.5719 (8.5719)	lr 5.182463e-05
after_epoch
epoch [68/100][1/1]	time 1.177 (1.177)	data 0.521 (0.521)	eta 0:00:37	loss 9.8529 (9.8529)	lr 4.909586e-05
after_epoch
epoch [69/100][1/1]	time 1.200 (1.200)	data 0.550 (0.550)	eta 0:00:37	loss 8.3492 (8.3492)	lr 4.641732e-05
after_epoch
epoch [70/100][1/1]	time 1.174 (1.174)	data 0.517 (0.517)	eta 0:00:35	loss 7.9619 (7.9619)	lr 4.379166e-05
after_epoch
epoch [71/100][1/1]	time 1.160 (1.160)	data 0.511 (0.511)	eta 0:00:33	loss 9.0590 (9.0590)	lr 4.122147e-05
after_epoch
epoch [72/100][1/1]	time 1.182 (1.182)	data 0.522 (0.522)	eta 0:00:33	loss 9.3856 (9.3856)	lr 3.870929e-05
after_epoch
epoch [73/100][1/1]	time 1.176 (1.176)	data 0.521 (0.521)	eta 0:00:31	loss 7.7217 (7.7217)	lr 3.625760e-05
after_epoch
epoch [74/100][1/1]	time 1.183 (1.183)	data 0.529 (0.529)	eta 0:00:30	loss 8.0107 (8.0107)	lr 3.386881e-05
after_epoch
epoch [75/100][1/1]	time 1.200 (1.200)	data 0.552 (0.552)	eta 0:00:30	loss 9.7710 (9.7710)	lr 3.154529e-05
after_epoch
epoch [76/100][1/1]	time 1.191 (1.191)	data 0.528 (0.528)	eta 0:00:28	loss 8.8140 (8.8140)	lr 2.928932e-05
after_epoch
epoch [77/100][1/1]	time 1.180 (1.180)	data 0.523 (0.523)	eta 0:00:27	loss 9.2710 (9.2710)	lr 2.710314e-05
after_epoch
epoch [78/100][1/1]	time 1.181 (1.181)	data 0.519 (0.519)	eta 0:00:25	loss 9.8453 (9.8453)	lr 2.498889e-05
after_epoch
epoch [79/100][1/1]	time 1.178 (1.178)	data 0.521 (0.521)	eta 0:00:24	loss 8.9710 (8.9710)	lr 2.294868e-05
after_epoch
epoch [80/100][1/1]	time 1.244 (1.244)	data 0.589 (0.589)	eta 0:00:24	loss 8.8743 (8.8743)	lr 2.098450e-05
after_epoch
epoch [81/100][1/1]	time 1.441 (1.441)	data 0.780 (0.780)	eta 0:00:27	loss 8.5431 (8.5431)	lr 1.909830e-05
after_epoch
epoch [82/100][1/1]	time 1.190 (1.190)	data 0.532 (0.532)	eta 0:00:21	loss 8.5596 (8.5596)	lr 1.729194e-05
after_epoch
epoch [83/100][1/1]	time 1.167 (1.167)	data 0.521 (0.521)	eta 0:00:19	loss 8.3469 (8.3469)	lr 1.556721e-05
after_epoch
epoch [84/100][1/1]	time 1.171 (1.171)	data 0.528 (0.528)	eta 0:00:18	loss 9.0000 (9.0000)	lr 1.392580e-05
after_epoch
epoch [85/100][1/1]	time 1.197 (1.197)	data 0.540 (0.540)	eta 0:00:17	loss 9.8785 (9.8785)	lr 1.236933e-05
after_epoch
epoch [86/100][1/1]	time 1.150 (1.150)	data 0.503 (0.503)	eta 0:00:16	loss 8.6861 (8.6861)	lr 1.089935e-05
after_epoch
epoch [87/100][1/1]	time 1.172 (1.172)	data 0.528 (0.528)	eta 0:00:15	loss 8.5329 (8.5329)	lr 9.517295e-06
after_epoch
epoch [88/100][1/1]	time 1.189 (1.189)	data 0.530 (0.530)	eta 0:00:14	loss 9.3902 (9.3902)	lr 8.224537e-06
after_epoch
epoch [89/100][1/1]	time 1.175 (1.175)	data 0.511 (0.511)	eta 0:00:12	loss 9.2091 (9.2091)	lr 7.022351e-06
after_epoch
epoch [90/100][1/1]	time 1.199 (1.199)	data 0.537 (0.537)	eta 0:00:11	loss 8.3546 (8.3546)	lr 5.911923e-06
after_epoch
epoch [91/100][1/1]	time 1.157 (1.157)	data 0.500 (0.500)	eta 0:00:10	loss 9.2281 (9.2281)	lr 4.894348e-06
after_epoch
epoch [92/100][1/1]	time 1.196 (1.196)	data 0.554 (0.554)	eta 0:00:09	loss 10.1748 (10.1748)	lr 3.970631e-06
after_epoch
epoch [93/100][1/1]	time 1.186 (1.186)	data 0.532 (0.532)	eta 0:00:08	loss 8.5765 (8.5765)	lr 3.141684e-06
after_epoch
epoch [94/100][1/1]	time 1.186 (1.186)	data 0.538 (0.538)	eta 0:00:07	loss 8.7273 (8.7273)	lr 2.408324e-06
after_epoch
epoch [95/100][1/1]	time 1.185 (1.185)	data 0.530 (0.530)	eta 0:00:05	loss 9.9323 (9.9323)	lr 1.771275e-06
after_epoch
epoch [96/100][1/1]	time 4.100 (4.100)	data 3.436 (3.436)	eta 0:00:16	loss 9.5381 (9.5381)	lr 1.231166e-06
after_epoch
epoch [97/100][1/1]	time 6.499 (6.499)	data 5.822 (5.822)	eta 0:00:19	loss 9.5823 (9.5823)	lr 7.885299e-07
after_epoch
epoch [98/100][1/1]	time 4.064 (4.064)	data 3.408 (3.408)	eta 0:00:08	loss 8.8186 (8.8186)	lr 4.438035e-07
after_epoch
epoch [99/100][1/1]	time 2.371 (2.371)	data 1.711 (1.711)	eta 0:00:02	loss 8.5297 (8.5297)	lr 1.973272e-07
after_epoch
epoch [100/100][1/1]	time 2.985 (2.985)	data 2.321 (2.321)	eta 0:00:00	loss 8.4944 (8.4944)	lr 4.934396e-08
after_epoch
Checkpoint saved to "output/baseline/Caption_distill_double/rn50/nctx16_cscFalse_ctpend/seed1/prompt_learner/model.pth.tar-100"
Finished training
Do evaluation on test set
Elapsed: 0:10:49
